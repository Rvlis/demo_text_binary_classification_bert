{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting Movie Reviews with BERT on TF Hub.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3613jvsc74a57bd0e63f34a48533c55a6ac08f2cff8b19c23be3b1cfbb0a4c4ba0a1f4fd4bcb88bf",
      "display_name": "Python 3.6.13 64-bit ('bert': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0a4mTk9o1Qg"
      },
      "source": [
        "# Copyright 2019 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ"
      },
      "source": [
        "#Predicting Movie Review Sentiment with BERT on TF Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYrZKaHwV81"
      },
      "source": [
        "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
        "\n",
        "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe204d53-e947-4a17-92dc-79ce87bb25bd"
      },
      "source": [
        "# !pip install bert-tensorflow==1.0.1\n",
        "# !pip install tensorflow==1.15.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9568fa8c-dfe7-45c8-bd72-68721b00469e"
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From D:\\Anaconda3\\envs\\bert\\lib\\site-packages\\bert\\optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e22e533-73df-40f7-8c2a-03deaedc3eac"
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'OUTPUT_DIR'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "# DO_DELETE = True #@param {type:\"boolean\"}\n",
        "# #@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "# USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "# BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
        "\n",
        "# if USE_BUCKET:\n",
        "#   OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "#   from google.colab import auth\n",
        "#   auth.authenticate_user()\n",
        "\n",
        "# if DO_DELETE:\n",
        "#   try:\n",
        "#     tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "#   except:\n",
        "#     # Doesn't matter if the directory didn't exist\n",
        "#     pass\n",
        "# tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "# print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr"
      },
      "source": [
        "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom_ff20gyy6"
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  print(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "  return train_df, test_df\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abfwdn-g135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e517632-e922-428b-954f-61d12836a2f8"
      },
      "source": [
        "train, test = download_and_load_datasets()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 29s 0us/step\n",
            "C:\\Users\\ayyk\\.keras\\datasets\\aclImdb\\train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf"
      },
      "source": [
        "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV"
      },
      "source": [
        "train = train.sample(5000)\n",
        "test = test.sample(5000)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRQM8pDi8xI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a6923644-aba3-4d75-ad1b-247e4764f6cf"
      },
      "source": [
        "train.columns\n",
        "train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                sentence sentiment  polarity\n",
              "4284   This film had such promise!! What a great idea...         4         0\n",
              "16765  I don't see enough TV game shows to understand...         3         0\n",
              "16180  This is by far the most incredible movie I hav...        10         1\n",
              "5311   This movie was a confusing piece of garbage. Y...         4         0\n",
              "9753   I initially bought this DVD because it had SRK...         8         1"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>sentiment</th>\n      <th>polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4284</th>\n      <td>This film had such promise!! What a great idea...</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16765</th>\n      <td>I don't see enough TV game shows to understand...</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16180</th>\n      <td>This is by far the most incredible movie I hav...</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5311</th>\n      <td>This movie was a confusing piece of garbage. Y...</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9753</th>\n      <td>I initially bought this DVD because it had SRK...</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz"
      },
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it"
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'polarity'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z"
      },
      "source": [
        "# Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bdc937-2356-4651-e4af-f1baff54dc16"
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "print(train_InputExamples)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4284     <bert.run_classifier.InputExample object at 0x...\n16765    <bert.run_classifier.InputExample object at 0x...\n16180    <bert.run_classifier.InputExample object at 0x...\n5311     <bert.run_classifier.InputExample object at 0x...\n9753     <bert.run_classifier.InputExample object at 0x...\n                               ...                        \n19316    <bert.run_classifier.InputExample object at 0x...\n17551    <bert.run_classifier.InputExample object at 0x...\n13731    <bert.run_classifier.InputExample object at 0x...\n24712    <bert.run_classifier.InputExample object at 0x...\n20731    <bert.run_classifier.InputExample object at 0x...\nLength: 5000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZWZtKxObjh"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855441a1-71b1-4819-8955-7915f69cbd2d"
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    print(tokenization_info)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "{'vocab_file': <tf.Tensor 'module_apply_tokenization_info/vocab_file:0' shape=() dtype=string>, 'do_lower_case': <tf.Tensor 'module_apply_tokenization_info/Const:0' shape=() dtype=bool>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm"
      },
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d6c9a2-c4dd-49cd-fee6-399c36590173"
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b328343a-f0e2-4f14-8cda-82ecfe657579"
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "print(len(train_features))\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From D:\\Anaconda3\\envs\\bert\\lib\\site-packages\\bert\\run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "WARNING:tensorflow:From D:\\Anaconda3\\envs\\bert\\lib\\site-packages\\bert\\run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Writing example 0 of 5000\n",
            "INFO:tensorflow:Writing example 0 of 5000\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] this film had such promise ! ! what a great idea , an under ##dog paint ##ball team struggling for recognition and personal glory , only to lose it ' s speed due to bad dial ##oge , poor editing and a half - written story . the characters in the beginning were interesting , only to lose steam half way through to become one dimensional people sp ##utter ##ing out tired one - liner ##s . < br / > < br / > maybe if they spent some more time on the story and dial ##oge it would have been a great movie , instead of a almost after ##th ##ough ##t effort . [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] this film had such promise ! ! what a great idea , an under ##dog paint ##ball team struggling for recognition and personal glory , only to lose it ' s speed due to bad dial ##oge , poor editing and a half - written story . the characters in the beginning were interesting , only to lose steam half way through to become one dimensional people sp ##utter ##ing out tired one - liner ##s . < br / > < br / > maybe if they spent some more time on the story and dial ##oge it would have been a great movie , instead of a almost after ##th ##ough ##t effort . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2023 2143 2018 2107 4872 999 999 2054 1037 2307 2801 1010 2019 2104 16168 6773 7384 2136 8084 2005 5038 1998 3167 8294 1010 2069 2000 4558 2009 1005 1055 3177 2349 2000 2919 13764 23884 1010 3532 9260 1998 1037 2431 1011 2517 2466 1012 1996 3494 1999 1996 2927 2020 5875 1010 2069 2000 4558 5492 2431 2126 2083 2000 2468 2028 8789 2111 11867 26878 2075 2041 5458 2028 1011 11197 2015 1012 1026 7987 1013 1028 1026 7987 1013 1028 2672 2065 2027 2985 2070 2062 2051 2006 1996 2466 1998 13764 23884 2009 2052 2031 2042 1037 2307 3185 1010 2612 1997 1037 2471 2044 2705 10593 2102 3947 1012 102 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_ids: 101 2023 2143 2018 2107 4872 999 999 2054 1037 2307 2801 1010 2019 2104 16168 6773 7384 2136 8084 2005 5038 1998 3167 8294 1010 2069 2000 4558 2009 1005 1055 3177 2349 2000 2919 13764 23884 1010 3532 9260 1998 1037 2431 1011 2517 2466 1012 1996 3494 1999 1996 2927 2020 5875 1010 2069 2000 4558 5492 2431 2126 2083 2000 2468 2028 8789 2111 11867 26878 2075 2041 5458 2028 1011 11197 2015 1012 1026 7987 1013 1028 1026 7987 1013 1028 2672 2065 2027 2985 2070 2062 2051 2006 1996 2466 1998 13764 23884 2009 2052 2031 2042 1037 2307 3185 1010 2612 1997 1037 2471 2044 2705 10593 2102 3947 1012 102 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] i don ' t see enough tv game shows to understand the attraction of show me the money , but i suppose it holds some appeal for und ##eman ##ding audiences . ostensibly a quiz show , it offers contestants huge sums of money for answering a few simple questions . however , its quiz elements play only a small part in the proceedings , which i find tor ##tu ##ously complicated . for example , before answering a question , a contestant selects which question is to be asked by choosing from among random \" a , \" \" b , \" or \" c \" choices . does this serve any purpose other than to slow the game down ? it would be a [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] i don ' t see enough tv game shows to understand the attraction of show me the money , but i suppose it holds some appeal for und ##eman ##ding audiences . ostensibly a quiz show , it offers contestants huge sums of money for answering a few simple questions . however , its quiz elements play only a small part in the proceedings , which i find tor ##tu ##ously complicated . for example , before answering a question , a contestant selects which question is to be asked by choosing from among random \" a , \" \" b , \" or \" c \" choices . does this serve any purpose other than to slow the game down ? it would be a [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1045 2123 1005 1056 2156 2438 2694 2208 3065 2000 3305 1996 8432 1997 2265 2033 1996 2769 1010 2021 1045 6814 2009 4324 2070 5574 2005 6151 16704 4667 9501 1012 23734 1037 19461 2265 1010 2009 4107 10584 4121 20571 1997 2769 2005 10739 1037 2261 3722 3980 1012 2174 1010 2049 19461 3787 2377 2069 1037 2235 2112 1999 1996 8931 1010 2029 1045 2424 17153 8525 13453 8552 1012 2005 2742 1010 2077 10739 1037 3160 1010 1037 10832 27034 2029 3160 2003 2000 2022 2356 2011 10549 2013 2426 6721 1000 1037 1010 1000 1000 1038 1010 1000 2030 1000 1039 1000 9804 1012 2515 2023 3710 2151 3800 2060 2084 2000 4030 1996 2208 2091 1029 2009 2052 2022 1037 102\n",
            "INFO:tensorflow:input_ids: 101 1045 2123 1005 1056 2156 2438 2694 2208 3065 2000 3305 1996 8432 1997 2265 2033 1996 2769 1010 2021 1045 6814 2009 4324 2070 5574 2005 6151 16704 4667 9501 1012 23734 1037 19461 2265 1010 2009 4107 10584 4121 20571 1997 2769 2005 10739 1037 2261 3722 3980 1012 2174 1010 2049 19461 3787 2377 2069 1037 2235 2112 1999 1996 8931 1010 2029 1045 2424 17153 8525 13453 8552 1012 2005 2742 1010 2077 10739 1037 3160 1010 1037 10832 27034 2029 3160 2003 2000 2022 2356 2011 10549 2013 2426 6721 1000 1037 1010 1000 1000 1038 1010 1000 2030 1000 1039 1000 9804 1012 2515 2023 3710 2151 3800 2060 2084 2000 4030 1996 2208 2091 1029 2009 2052 2022 1037 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] this is by far the most incredible movie i have seen in a long time . the actors gave wonderful portrayal ##s of the characters in the movie . the story was accurately portrayed . the story starts out with a young woman from the british isles and her father traveling by steamboat to na ##u ##vo ##o , illinois . she has become a member of the lds church and he has not . he thinks she is ridiculous for making the trip and is disco ##ura ##ging . she encourages him to read about joseph smith , the prophet . this is where the story of the prophet joseph smith begins . the movie accurately portrays his life and some of the history of [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] this is by far the most incredible movie i have seen in a long time . the actors gave wonderful portrayal ##s of the characters in the movie . the story was accurately portrayed . the story starts out with a young woman from the british isles and her father traveling by steamboat to na ##u ##vo ##o , illinois . she has become a member of the lds church and he has not . he thinks she is ridiculous for making the trip and is disco ##ura ##ging . she encourages him to read about joseph smith , the prophet . this is where the story of the prophet joseph smith begins . the movie accurately portrays his life and some of the history of [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2023 2003 2011 2521 1996 2087 9788 3185 1045 2031 2464 1999 1037 2146 2051 1012 1996 5889 2435 6919 13954 2015 1997 1996 3494 1999 1996 3185 1012 1996 2466 2001 14125 6791 1012 1996 2466 4627 2041 2007 1037 2402 2450 2013 1996 2329 14195 1998 2014 2269 7118 2011 24897 2000 6583 2226 6767 2080 1010 4307 1012 2016 2038 2468 1037 2266 1997 1996 17627 2277 1998 2002 2038 2025 1012 2002 6732 2016 2003 9951 2005 2437 1996 4440 1998 2003 12532 4648 4726 1012 2016 16171 2032 2000 3191 2055 3312 3044 1010 1996 12168 1012 2023 2003 2073 1996 2466 1997 1996 12168 3312 3044 4269 1012 1996 3185 14125 17509 2010 2166 1998 2070 1997 1996 2381 1997 102\n",
            "INFO:tensorflow:input_ids: 101 2023 2003 2011 2521 1996 2087 9788 3185 1045 2031 2464 1999 1037 2146 2051 1012 1996 5889 2435 6919 13954 2015 1997 1996 3494 1999 1996 3185 1012 1996 2466 2001 14125 6791 1012 1996 2466 4627 2041 2007 1037 2402 2450 2013 1996 2329 14195 1998 2014 2269 7118 2011 24897 2000 6583 2226 6767 2080 1010 4307 1012 2016 2038 2468 1037 2266 1997 1996 17627 2277 1998 2002 2038 2025 1012 2002 6732 2016 2003 9951 2005 2437 1996 4440 1998 2003 12532 4648 4726 1012 2016 16171 2032 2000 3191 2055 3312 3044 1010 1996 12168 1012 2023 2003 2073 1996 2466 1997 1996 12168 3312 3044 4269 1012 1996 3185 14125 17509 2010 2166 1998 2070 1997 1996 2381 1997 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] this movie was a confusing piece of garbage . you never knew what was going on . the characters were poorly written and for the most part they were totally un ##sy ##mp ##ath ##etic except for gus ( played master ##fully by george ea ##ds ) . i hate this movie but compared to others ( dark harvest , dracula ' s curse ) it should have won an academy award . it was particularly sad to see a talented actor like george ea ##ds in such a disgrace ##ful and tack ##y film . lifetime you have sunken to whole new low . someone needs to make sure that this director never works in movies again . also was this supposed to be a [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] this movie was a confusing piece of garbage . you never knew what was going on . the characters were poorly written and for the most part they were totally un ##sy ##mp ##ath ##etic except for gus ( played master ##fully by george ea ##ds ) . i hate this movie but compared to others ( dark harvest , dracula ' s curse ) it should have won an academy award . it was particularly sad to see a talented actor like george ea ##ds in such a disgrace ##ful and tack ##y film . lifetime you have sunken to whole new low . someone needs to make sure that this director never works in movies again . also was this supposed to be a [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2023 3185 2001 1037 16801 3538 1997 13044 1012 2017 2196 2354 2054 2001 2183 2006 1012 1996 3494 2020 9996 2517 1998 2005 1996 2087 2112 2027 2020 6135 4895 6508 8737 8988 16530 3272 2005 12670 1006 2209 3040 7699 2011 2577 19413 5104 1007 1012 1045 5223 2023 3185 2021 4102 2000 2500 1006 2601 11203 1010 18500 1005 1055 8364 1007 2009 2323 2031 2180 2019 2914 2400 1012 2009 2001 3391 6517 2000 2156 1037 10904 3364 2066 2577 19413 5104 1999 2107 1037 29591 3993 1998 26997 2100 2143 1012 6480 2017 2031 23470 2000 2878 2047 2659 1012 2619 3791 2000 2191 2469 2008 2023 2472 2196 2573 1999 5691 2153 1012 2036 2001 2023 4011 2000 2022 1037 102\n",
            "INFO:tensorflow:input_ids: 101 2023 3185 2001 1037 16801 3538 1997 13044 1012 2017 2196 2354 2054 2001 2183 2006 1012 1996 3494 2020 9996 2517 1998 2005 1996 2087 2112 2027 2020 6135 4895 6508 8737 8988 16530 3272 2005 12670 1006 2209 3040 7699 2011 2577 19413 5104 1007 1012 1045 5223 2023 3185 2021 4102 2000 2500 1006 2601 11203 1010 18500 1005 1055 8364 1007 2009 2323 2031 2180 2019 2914 2400 1012 2009 2001 3391 6517 2000 2156 1037 10904 3364 2066 2577 19413 5104 1999 2107 1037 29591 3993 1998 26997 2100 2143 1012 6480 2017 2031 23470 2000 2878 2047 2659 1012 2619 3791 2000 2191 2469 2008 2023 2472 2196 2573 1999 5691 2153 1012 2036 2001 2023 4011 2000 2022 1037 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] i initially bought this dvd because it had sr ##k and ai ##sh ##war ##ya rai on the cover and i thought , hey ! another film starring ai ##shu and shah ru ##kh , little did i know that ai ##sh ##war ##ya would only appear in an item number in the last quarter of the film in a song which she shares with sr ##k and helps introduce his character who is in the film for about just 15 minutes . sha ##kti is a film about a mother ' s love and endurance . it ' s a film about transformations , ignorance , coming of age , stepping into the know and embracing the harsh realities of life . the item number [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] i initially bought this dvd because it had sr ##k and ai ##sh ##war ##ya rai on the cover and i thought , hey ! another film starring ai ##shu and shah ru ##kh , little did i know that ai ##sh ##war ##ya would only appear in an item number in the last quarter of the film in a song which she shares with sr ##k and helps introduce his character who is in the film for about just 15 minutes . sha ##kti is a film about a mother ' s love and endurance . it ' s a film about transformations , ignorance , coming of age , stepping into the know and embracing the harsh realities of life . the item number [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1045 3322 4149 2023 4966 2138 2009 2018 5034 2243 1998 9932 4095 9028 3148 15547 2006 1996 3104 1998 1045 2245 1010 4931 999 2178 2143 4626 9932 14235 1998 7890 21766 10023 1010 2210 2106 1045 2113 2008 9932 4095 9028 3148 2052 2069 3711 1999 2019 8875 2193 1999 1996 2197 4284 1997 1996 2143 1999 1037 2299 2029 2016 6661 2007 5034 2243 1998 7126 8970 2010 2839 2040 2003 1999 1996 2143 2005 2055 2074 2321 2781 1012 21146 22462 2003 1037 2143 2055 1037 2388 1005 1055 2293 1998 14280 1012 2009 1005 1055 1037 2143 2055 21865 1010 18173 1010 2746 1997 2287 1010 9085 2046 1996 2113 1998 23581 1996 8401 22213 1997 2166 1012 1996 8875 2193 102\n",
            "INFO:tensorflow:input_ids: 101 1045 3322 4149 2023 4966 2138 2009 2018 5034 2243 1998 9932 4095 9028 3148 15547 2006 1996 3104 1998 1045 2245 1010 4931 999 2178 2143 4626 9932 14235 1998 7890 21766 10023 1010 2210 2106 1045 2113 2008 9932 4095 9028 3148 2052 2069 3711 1999 2019 8875 2193 1999 1996 2197 4284 1997 1996 2143 1999 1037 2299 2029 2016 6661 2007 5034 2243 1998 7126 8970 2010 2839 2040 2003 1999 1996 2143 2005 2055 2074 2321 2781 1012 21146 22462 2003 1037 2143 2055 1037 2388 1005 1055 2293 1998 14280 1012 2009 1005 1055 1037 2143 2055 21865 1010 18173 1010 2746 1997 2287 1010 9085 2046 1996 2113 1998 23581 1996 8401 22213 1997 2166 1012 1996 8875 2193 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "5000\n",
            "INFO:tensorflow:Writing example 0 of 5000\n",
            "INFO:tensorflow:Writing example 0 of 5000\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] the emergence of quentin tara ##ntino and his dubious influence on the likes of guy ritchie may have triggered the wave of app ##all ##ing british gangster flick ##s we ' ve been bequeathed over the past few years , but one of our most famous acting exports only serves to per ##pet ##uate the cycle by lending his considerable name to trash like this . i only wish he ' d taken a moment to consider before choosing this project for the same reasons of personal gain he admits he often employs . it ' s not only st ##if ##ling his talent , but possibly the promise of future original ##ity from british films . < br / > < br / > not [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] the emergence of quentin tara ##ntino and his dubious influence on the likes of guy ritchie may have triggered the wave of app ##all ##ing british gangster flick ##s we ' ve been bequeathed over the past few years , but one of our most famous acting exports only serves to per ##pet ##uate the cycle by lending his considerable name to trash like this . i only wish he ' d taken a moment to consider before choosing this project for the same reasons of personal gain he admits he often employs . it ' s not only st ##if ##ling his talent , but possibly the promise of future original ##ity from british films . < br / > < br / > not [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1996 14053 1997 15969 10225 25318 1998 2010 22917 3747 2006 1996 7777 1997 3124 20404 2089 2031 13330 1996 4400 1997 10439 8095 2075 2329 20067 17312 2015 2057 1005 2310 2042 27180 2058 1996 2627 2261 2086 1010 2021 2028 1997 2256 2087 3297 3772 14338 2069 4240 2000 2566 22327 20598 1996 5402 2011 18435 2010 6196 2171 2000 11669 2066 2023 1012 1045 2069 4299 2002 1005 1040 2579 1037 2617 2000 5136 2077 10549 2023 2622 2005 1996 2168 4436 1997 3167 5114 2002 14456 2002 2411 13495 1012 2009 1005 1055 2025 2069 2358 10128 2989 2010 5848 1010 2021 4298 1996 4872 1997 2925 2434 3012 2013 2329 3152 1012 1026 7987 1013 1028 1026 7987 1013 1028 2025 102\n",
            "INFO:tensorflow:input_ids: 101 1996 14053 1997 15969 10225 25318 1998 2010 22917 3747 2006 1996 7777 1997 3124 20404 2089 2031 13330 1996 4400 1997 10439 8095 2075 2329 20067 17312 2015 2057 1005 2310 2042 27180 2058 1996 2627 2261 2086 1010 2021 2028 1997 2256 2087 3297 3772 14338 2069 4240 2000 2566 22327 20598 1996 5402 2011 18435 2010 6196 2171 2000 11669 2066 2023 1012 1045 2069 4299 2002 1005 1040 2579 1037 2617 2000 5136 2077 10549 2023 2622 2005 1996 2168 4436 1997 3167 5114 2002 14456 2002 2411 13495 1012 2009 1005 1055 2025 2069 2358 10128 2989 2010 5848 1010 2021 4298 1996 4872 1997 2925 2434 3012 2013 2329 3152 1012 1026 7987 1013 1028 1026 7987 1013 1028 2025 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] i ' m not really much of an abbott & costello fan ( although i do enjoy \" who ' s on first \" ) and , to be honest , there wasn ' t much in this movie that would inspire me to watch any more of their work . it wasn ' t really bad . it had some mildly amusing scenes , and actually a very convincing giant played by buddy bae ##r , but somehow , given the fame of the duo and the esteem in which they ' re generally held , i have to say i was expecting more . as the story goes , the pair stumble into a baby ##sit ##ting job , and during the reading of [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] i ' m not really much of an abbott & costello fan ( although i do enjoy \" who ' s on first \" ) and , to be honest , there wasn ' t much in this movie that would inspire me to watch any more of their work . it wasn ' t really bad . it had some mildly amusing scenes , and actually a very convincing giant played by buddy bae ##r , but somehow , given the fame of the duo and the esteem in which they ' re generally held , i have to say i was expecting more . as the story goes , the pair stumble into a baby ##sit ##ting job , and during the reading of [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1045 1005 1049 2025 2428 2172 1997 2019 14455 1004 21015 5470 1006 2348 1045 2079 5959 1000 2040 1005 1055 2006 2034 1000 1007 1998 1010 2000 2022 7481 1010 2045 2347 1005 1056 2172 1999 2023 3185 2008 2052 18708 2033 2000 3422 2151 2062 1997 2037 2147 1012 2009 2347 1005 1056 2428 2919 1012 2009 2018 2070 19499 19142 5019 1010 1998 2941 1037 2200 13359 5016 2209 2011 8937 25818 2099 1010 2021 5064 1010 2445 1996 4476 1997 1996 6829 1998 1996 19593 1999 2029 2027 1005 2128 3227 2218 1010 1045 2031 2000 2360 1045 2001 8074 2062 1012 2004 1996 2466 3632 1010 1996 3940 21811 2046 1037 3336 28032 3436 3105 1010 1998 2076 1996 3752 1997 102\n",
            "INFO:tensorflow:input_ids: 101 1045 1005 1049 2025 2428 2172 1997 2019 14455 1004 21015 5470 1006 2348 1045 2079 5959 1000 2040 1005 1055 2006 2034 1000 1007 1998 1010 2000 2022 7481 1010 2045 2347 1005 1056 2172 1999 2023 3185 2008 2052 18708 2033 2000 3422 2151 2062 1997 2037 2147 1012 2009 2347 1005 1056 2428 2919 1012 2009 2018 2070 19499 19142 5019 1010 1998 2941 1037 2200 13359 5016 2209 2011 8937 25818 2099 1010 2021 5064 1010 2445 1996 4476 1997 1996 6829 1998 1996 19593 1999 2029 2027 1005 2128 3227 2218 1010 1045 2031 2000 2360 1045 2001 8074 2062 1012 2004 1996 2466 3632 1010 1996 3940 21811 2046 1037 3336 28032 3436 3105 1010 1998 2076 1996 3752 1997 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] like another user i got this cheap - i thought . 85 k ##rone ##rs ( £ ##8 ) . although not worth that amount of money it is a total classic that i - like the other users - first saw when i was a kid and was looking forward to seeing again some 20 years after . the story is amazingly thin ( which is why it might have worked for kids ) , but all the radio language and truck ##er stuff makes up for it . it ' ll look good on my dvd shelf in years to come . first viewing ( 1980 ) : 10 / 10 . second viewing ( 2002 ) : 4 / 10 [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] like another user i got this cheap - i thought . 85 k ##rone ##rs ( £ ##8 ) . although not worth that amount of money it is a total classic that i - like the other users - first saw when i was a kid and was looking forward to seeing again some 20 years after . the story is amazingly thin ( which is why it might have worked for kids ) , but all the radio language and truck ##er stuff makes up for it . it ' ll look good on my dvd shelf in years to come . first viewing ( 1980 ) : 10 / 10 . second viewing ( 2002 ) : 4 / 10 [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2066 2178 5310 1045 2288 2023 10036 1011 1045 2245 1012 5594 1047 20793 2869 1006 1069 2620 1007 1012 2348 2025 4276 2008 3815 1997 2769 2009 2003 1037 2561 4438 2008 1045 1011 2066 1996 2060 5198 1011 2034 2387 2043 1045 2001 1037 4845 1998 2001 2559 2830 2000 3773 2153 2070 2322 2086 2044 1012 1996 2466 2003 29350 4857 1006 2029 2003 2339 2009 2453 2031 2499 2005 4268 1007 1010 2021 2035 1996 2557 2653 1998 4744 2121 4933 3084 2039 2005 2009 1012 2009 1005 2222 2298 2204 2006 2026 4966 11142 1999 2086 2000 2272 1012 2034 10523 1006 3150 1007 1024 2184 1013 2184 1012 2117 10523 1006 2526 1007 1024 1018 1013 2184 102 0 0 0\n",
            "INFO:tensorflow:input_ids: 101 2066 2178 5310 1045 2288 2023 10036 1011 1045 2245 1012 5594 1047 20793 2869 1006 1069 2620 1007 1012 2348 2025 4276 2008 3815 1997 2769 2009 2003 1037 2561 4438 2008 1045 1011 2066 1996 2060 5198 1011 2034 2387 2043 1045 2001 1037 4845 1998 2001 2559 2830 2000 3773 2153 2070 2322 2086 2044 1012 1996 2466 2003 29350 4857 1006 2029 2003 2339 2009 2453 2031 2499 2005 4268 1007 1010 2021 2035 1996 2557 2653 1998 4744 2121 4933 3084 2039 2005 2009 1012 2009 1005 2222 2298 2204 2006 2026 4966 11142 1999 2086 2000 2272 1012 2034 10523 1006 3150 1007 1024 2184 1013 2184 1012 2117 10523 1006 2526 1007 1024 1018 1013 2184 102 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] great movie , great actors , great soundtrack ! i loved it ! settings are perfect , dialogues , situations , storyline . . . all together mixed to give this masterpiece ! cl ##oon ##ey and tu ##rt ##ur ##ro are magnificent and the so ##ggy bottom boys are simply charming and con ##tag ##ious with their music ! : ) [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] great movie , great actors , great soundtrack ! i loved it ! settings are perfect , dialogues , situations , storyline . . . all together mixed to give this masterpiece ! cl ##oon ##ey and tu ##rt ##ur ##ro are magnificent and the so ##ggy bottom boys are simply charming and con ##tag ##ious with their music ! : ) [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2307 3185 1010 2307 5889 1010 2307 6050 999 1045 3866 2009 999 10906 2024 3819 1010 22580 1010 8146 1010 9994 1012 1012 1012 2035 2362 3816 2000 2507 2023 17743 999 18856 7828 3240 1998 10722 5339 3126 3217 2024 12047 1998 1996 2061 22772 3953 3337 2024 3432 11951 1998 9530 15900 6313 2007 2037 2189 999 1024 1007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_ids: 101 2307 3185 1010 2307 5889 1010 2307 6050 999 1045 3866 2009 999 10906 2024 3819 1010 22580 1010 8146 1010 9994 1012 1012 1012 2035 2362 3816 2000 2507 2023 17743 999 18856 7828 3240 1998 10722 5339 3126 3217 2024 12047 1998 1996 2061 22772 3953 3337 2024 3432 11951 1998 9530 15900 6313 2007 2037 2189 999 1024 1007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] i was looking forward to the guardian , but when i walked into the theater i wasn ' t really in the mood for it at that particular time . it ' s kind of like the olive garden - i like it , but i have to be in the right minds ##et to thoroughly enjoy it . < br / > < br / > i ' m not exactly sure what was damp ##ening my spirit . the trailers looked good , but the water theme was giving me bad flashbacks to the last kevin cost ##ner movie that dealt with the subject - water ##world . plus , despite the promise ashton ku ##tch ##er showed in the butterfly effect , i [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] i was looking forward to the guardian , but when i walked into the theater i wasn ' t really in the mood for it at that particular time . it ' s kind of like the olive garden - i like it , but i have to be in the right minds ##et to thoroughly enjoy it . < br / > < br / > i ' m not exactly sure what was damp ##ening my spirit . the trailers looked good , but the water theme was giving me bad flashbacks to the last kevin cost ##ner movie that dealt with the subject - water ##world . plus , despite the promise ashton ku ##tch ##er showed in the butterfly effect , i [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1045 2001 2559 2830 2000 1996 6697 1010 2021 2043 1045 2939 2046 1996 4258 1045 2347 1005 1056 2428 1999 1996 6888 2005 2009 2012 2008 3327 2051 1012 2009 1005 1055 2785 1997 2066 1996 9724 3871 1011 1045 2066 2009 1010 2021 1045 2031 2000 2022 1999 1996 2157 9273 3388 2000 12246 5959 2009 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 1005 1049 2025 3599 2469 2054 2001 10620 7406 2026 4382 1012 1996 21389 2246 2204 1010 2021 1996 2300 4323 2001 3228 2033 2919 28945 2000 1996 2197 4901 3465 3678 3185 2008 9411 2007 1996 3395 1011 2300 11108 1012 4606 1010 2750 1996 4872 13772 13970 10649 2121 3662 1999 1996 9112 3466 1010 1045 102\n",
            "INFO:tensorflow:input_ids: 101 1045 2001 2559 2830 2000 1996 6697 1010 2021 2043 1045 2939 2046 1996 4258 1045 2347 1005 1056 2428 1999 1996 6888 2005 2009 2012 2008 3327 2051 1012 2009 1005 1055 2785 1997 2066 1996 9724 3871 1011 1045 2066 2009 1010 2021 1045 2031 2000 2022 1999 1996 2157 9273 3388 2000 12246 5959 2009 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 1005 1049 2025 3599 2469 2054 2001 10620 7406 2026 4382 1012 1996 21389 2246 2204 1010 2021 1996 2300 4323 2001 3228 2033 2919 28945 2000 1996 2197 4901 3465 3678 3185 2008 9411 2007 1996 3395 1011 2300 11108 1012 4606 1010 2750 1996 4872 13772 13970 10649 2121 3662 1999 1996 9112 3466 1010 1045 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq"
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "  \n",
        "  print(bert_outputs.keys())\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW"
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8"
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 10\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b2cf5e-da60-4ca9-ec78-a2e011163323"
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "print(num_train_steps)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa"
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea7f56e-f7c8-49ab-e56f-94bc99e9a5f9"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027AF3E26940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027AF3E26940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO3RfG1DYLo"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K"
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-"
      },
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3075716-8133-4a79-bc96-893306f79cd9"
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning Training!\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "dict_keys(['sequence_output', 'pooled_output'])\n",
            "D:\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR\\model.ckpt-0\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR\\model.ckpt-0\n",
            "WARNING:tensorflow:From D:\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "WARNING:tensorflow:From D:\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR\\model.ckpt.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR\\model.ckpt.\n",
            "INFO:tensorflow:loss = 0.68037695, step = 0\n",
            "INFO:tensorflow:loss = 0.68037695, step = 0\n",
            "INFO:tensorflow:global_step/sec: 2.23208\n",
            "INFO:tensorflow:global_step/sec: 2.23208\n",
            "INFO:tensorflow:loss = 0.4309333, step = 100 (44.802 sec)\n",
            "INFO:tensorflow:loss = 0.4309333, step = 100 (44.802 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.58793\n",
            "INFO:tensorflow:global_step/sec: 2.58793\n",
            "INFO:tensorflow:loss = 0.3130148, step = 200 (38.640 sec)\n",
            "INFO:tensorflow:loss = 0.3130148, step = 200 (38.640 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.91145\n",
            "INFO:tensorflow:global_step/sec: 2.91145\n",
            "INFO:tensorflow:loss = 0.14278626, step = 300 (34.347 sec)\n",
            "INFO:tensorflow:loss = 0.14278626, step = 300 (34.347 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.95382\n",
            "INFO:tensorflow:global_step/sec: 2.95382\n",
            "INFO:tensorflow:loss = 0.3188401, step = 400 (33.855 sec)\n",
            "INFO:tensorflow:loss = 0.3188401, step = 400 (33.855 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 500 into OUTPUT_DIR\\model.ckpt.\n",
            "INFO:tensorflow:Saving checkpoints for 500 into OUTPUT_DIR\\model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.24297\n",
            "INFO:tensorflow:global_step/sec: 2.24297\n",
            "INFO:tensorflow:loss = 0.30068445, step = 500 (44.583 sec)\n",
            "INFO:tensorflow:loss = 0.30068445, step = 500 (44.583 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44644\n",
            "INFO:tensorflow:global_step/sec: 2.44644\n",
            "INFO:tensorflow:loss = 0.017417675, step = 600 (40.877 sec)\n",
            "INFO:tensorflow:loss = 0.017417675, step = 600 (40.877 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.92205\n",
            "INFO:tensorflow:global_step/sec: 2.92205\n",
            "INFO:tensorflow:loss = 0.01664057, step = 700 (34.223 sec)\n",
            "INFO:tensorflow:loss = 0.01664057, step = 700 (34.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.61126\n",
            "INFO:tensorflow:global_step/sec: 2.61126\n",
            "INFO:tensorflow:loss = 0.0055528856, step = 800 (38.295 sec)\n",
            "INFO:tensorflow:loss = 0.0055528856, step = 800 (38.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.49682\n",
            "INFO:tensorflow:global_step/sec: 2.49682\n",
            "INFO:tensorflow:loss = 0.006656519, step = 900 (40.051 sec)\n",
            "INFO:tensorflow:loss = 0.006656519, step = 900 (40.051 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into OUTPUT_DIR\\model.ckpt.\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into OUTPUT_DIR\\model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 2.34741\n",
            "INFO:tensorflow:global_step/sec: 2.34741\n",
            "INFO:tensorflow:loss = 0.0020446696, step = 1000 (42.600 sec)\n",
            "INFO:tensorflow:loss = 0.0020446696, step = 1000 (42.600 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.78567\n",
            "INFO:tensorflow:global_step/sec: 2.78567\n",
            "INFO:tensorflow:loss = 0.0033230311, step = 1100 (35.912 sec)\n",
            "INFO:tensorflow:loss = 0.0033230311, step = 1100 (35.912 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.90874\n",
            "INFO:tensorflow:global_step/sec: 2.90874\n",
            "INFO:tensorflow:loss = 0.0064493464, step = 1200 (34.365 sec)\n",
            "INFO:tensorflow:loss = 0.0064493464, step = 1200 (34.365 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.4038\n",
            "INFO:tensorflow:global_step/sec: 2.4038\n",
            "INFO:tensorflow:loss = 0.0026747263, step = 1300 (41.602 sec)\n",
            "INFO:tensorflow:loss = 0.0026747263, step = 1300 (41.602 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.44997\n",
            "INFO:tensorflow:global_step/sec: 2.44997\n",
            "INFO:tensorflow:loss = 0.0028851423, step = 1400 (40.816 sec)\n",
            "INFO:tensorflow:loss = 0.0028851423, step = 1400 (40.816 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1500 into OUTPUT_DIR\\model.ckpt.\n",
            "INFO:tensorflow:Saving checkpoints for 1500 into OUTPUT_DIR\\model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.0019690488.\n",
            "INFO:tensorflow:Loss for final step: 0.0019690488.\n",
            "Training took time  0:10:26.004904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx"
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "dd5482cd-c558-465f-c854-ec11a0175316"
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "dict_keys(['sequence_output', 'pooled_output'])\n",
            "D:\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-04-11T17:11:49Z\n",
            "INFO:tensorflow:Starting evaluation at 2021-04-11T17:11:49Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR\\model.ckpt-1500\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR\\model.ckpt-1500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2021-04-11-17:12:47\n",
            "INFO:tensorflow:Finished evaluation at 2021-04-11-17:12:47\n",
            "INFO:tensorflow:Saving dict for global step 1500: auc = 0.8641965, eval_accuracy = 0.8642, f1_score = 0.86268955, false_negatives = 336.0, false_positives = 343.0, global_step = 1500, loss = 0.5914863, precision = 0.8614701, recall = 0.8639125, true_negatives = 2188.0, true_positives = 2133.0\n",
            "INFO:tensorflow:Saving dict for global step 1500: auc = 0.8641965, eval_accuracy = 0.8642, f1_score = 0.86268955, false_negatives = 336.0, false_positives = 343.0, global_step = 1500, loss = 0.5914863, precision = 0.8614701, recall = 0.8639125, true_negatives = 2188.0, true_positives = 2133.0\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1500: OUTPUT_DIR\\model.ckpt-1500\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1500: OUTPUT_DIR\\model.ckpt-1500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.8641965,\n",
              " 'eval_accuracy': 0.8642,\n",
              " 'f1_score': 0.86268955,\n",
              " 'false_negatives': 336.0,\n",
              " 'false_positives': 343.0,\n",
              " 'global_step': 1500,\n",
              " 'loss': 0.5914863,\n",
              " 'precision': 0.8614701,\n",
              " 'recall': 0.8639125,\n",
              " 'true_negatives': 2188.0,\n",
              " 'true_positives': 2133.0}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B"
      },
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsrbTD2EJTVl"
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = [\"Negative\", \"Positive\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-thbodgih_VJ"
      },
      "source": [
        "pred_sentences = [\n",
        "  \"That movie was absolutely awful\",\n",
        "  \"The acting was a bit lacking\",\n",
        "  \"The film was creative and surprising\",\n",
        "  \"Absolutely fantastic!\"\n",
        "]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrZmvZySKQTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "3891fafb-a460-4eb8-fa6c-335a5bbc10e5"
      },
      "source": [
        "predictions = getPrediction(pred_sentences)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Writing example 0 of 4\n",
            "INFO:tensorflow:Writing example 0 of 4\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n",
            "INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n",
            "INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "dict_keys(['pooled_output', 'sequence_output'])\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR\\model.ckpt-1500\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR\\model.ckpt-1500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXkRiEBUqN3n"
      },
      "source": [
        "Voila! We have a sentiment classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERkTE8-7oQLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('That movie was absolutely awful',\n",
              "  array([-1.5331669e-03, -6.4811511e+00], dtype=float32),\n",
              "  'Negative'),\n",
              " ('The acting was a bit lacking',\n",
              "  array([-3.0632736e-03, -5.7897954e+00], dtype=float32),\n",
              "  'Negative'),\n",
              " ('The film was creative and surprising',\n",
              "  array([-5.5251665e+00, -3.9931573e-03], dtype=float32),\n",
              "  'Positive'),\n",
              " ('Absolutely fantastic!',\n",
              "  array([-5.7538357e+00, -3.1755755e-03], dtype=float32),\n",
              "  'Positive')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}